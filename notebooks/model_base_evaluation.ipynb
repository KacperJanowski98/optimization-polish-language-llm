{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf8d8f7e",
   "metadata": {},
   "source": [
    "# Polish Language Model Evaluation with Polish GLUE\n",
    "\n",
    "This notebook evaluates and compares three language models on Polish language tasks using the Polish GLUE (KLEJ) benchmark:\n",
    "\n",
    "1. **Bielik-11B-v2.3-Instruct** - Specialized Polish language model\n",
    "2. **Gemma-3-4B-IT** - Multilingual model from Google\n",
    "3. **Phi-4-mini-instruct** - Multilingual model from Microsoft\n",
    "\n",
    "The Polish GLUE (KLEJ) benchmark includes multiple tasks specifically designed to evaluate language models on Polish language understanding.\n",
    "\n",
    "## Tasks in Polish GLUE (KLEJ)\n",
    "\n",
    "1. **CDSC-E** - Compositional Distributional Semantics Corpus - Entailment task\n",
    "2. **CDSC-R** - Compositional Distributional Semantics Corpus - Relatedness task\n",
    "3. **CBD** - Cyberbullying Detection dataset\n",
    "4. **PolEmo2-in** - Sentiment analysis (in-domain)\n",
    "5. **PolEmo2-out** - Sentiment analysis (out-of-domain)\n",
    "6. **DYK** - Did You Know - Genuine question detection \n",
    "7. **PSC** - Polish Summaries Corpus\n",
    "8. **AR** - Allegro Reviews - Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecb06a5",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd91c5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add parent directory to path to allow importing from src\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "# Import necessary libraries\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Import project modules\n",
    "from src.utils import authenticate_huggingface, get_device\n",
    "from src.init_models import initialize_models, configure_generation_parameters\n",
    "from src.datasets import prepare_polish_datasets, create_task_specific_prompt\n",
    "from src.evaluation import evaluate_models\n",
    "\n",
    "# Configure visualization\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"viridis\")\n",
    "\n",
    "# Set up logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8da6c15",
   "metadata": {},
   "source": [
    "## Hugging Face Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c97ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get Hugging Face token from environment\n",
    "hf_token = os.getenv(\"HF_TOKEN\")  # Load from environment\n",
    "if not hf_token:\n",
    "    # If not found, ask for token input\n",
    "    import getpass\n",
    "    hf_token = os.getenv(\"HF_TOKEN\")  # Load from environment\n",
    "\n",
    "# Authenticate with Hugging Face\n",
    "authenticate_huggingface(hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f0a5cd",
   "metadata": {},
   "source": [
    "## Check Available Hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1c3d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Check number of GPUs\n",
    "if torch.cuda.is_available():\n",
    "    device_count = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs: {device_count}\")\n",
    "    \n",
    "    for i in range(device_count):\n",
    "        device_name = torch.cuda.get_device_name(i)\n",
    "        total_memory = torch.cuda.get_device_properties(i).total_memory / 1e9  # Convert to GB\n",
    "        print(f\"GPU {i}: {device_name} ({total_memory:.2f} GB)\")\n",
    "else:\n",
    "    print(\"No GPUs detected, using CPU\")\n",
    "\n",
    "# Determine the device to use\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d85e4a5",
   "metadata": {},
   "source": [
    "## Load Polish GLUE Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d5f350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a small subset of KLEJ datasets for initial testing\n",
    "# You can adjust max_samples for full evaluation later\n",
    "datasets = prepare_polish_datasets(\n",
    "    tasks=[\"klej\"],\n",
    "    # Select specific KLEJ tasks (comment out to load all)\n",
    "    klej_datasets=[\"cdsc-e\", \"polemo2-in\"],\n",
    "    max_samples=10  # Small number for testing, increase for full evaluation\n",
    ")\n",
    "\n",
    "# Add translation dataset as well\n",
    "translation_dataset = prepare_polish_datasets(\n",
    "    tasks=[\"translation\"],\n",
    "    max_samples=10\n",
    ")\n",
    "datasets.update(translation_dataset)\n",
    "\n",
    "# Display dataset information\n",
    "for task_name, dataset_info in datasets.items():\n",
    "    print(f\"\\nTask: {task_name}\")\n",
    "    print(f\"Description: {dataset_info['description']}\")\n",
    "    print(f\"Task type: {dataset_info['task_type']}\")\n",
    "    print(f\"Metric: {dataset_info['metric']}\")\n",
    "    print(f\"Dataset size: {len(dataset_info['dataset'])}\")\n",
    "    \n",
    "    # Show a sample example\n",
    "    print(\"\\nSample item:\")\n",
    "    sample = dataset_info['dataset'][0]\n",
    "    for key in dataset_info['keys']:\n",
    "        value = sample[key]\n",
    "        if isinstance(value, str) and len(value) > 100:\n",
    "            print(f\"  {key}: {value[:100]}...\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Show example prompt\n",
    "    prompt = create_task_specific_prompt(task_name, sample)\n",
    "    print(f\"\\nExample prompt:\\n{prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1bdfc5",
   "metadata": {},
   "source": [
    "## Initialize Models\n",
    "\n",
    "**Note**: Loading all models at once may exceed GPU memory. Consider loading them one by one as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b6f28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For initial testing, let's start with the smallest model (Phi-4-mini)\n",
    "models = initialize_models(\n",
    "    device=device,\n",
    "    load_in_8bit=True,  # Use 8-bit quantization to save memory\n",
    "    token= os.getenv(\"HF_TOKEN\")  # Load from environment\n",
    "    models_to_load=[\"phi\"]  # Start with just one model\n",
    ")\n",
    "\n",
    "# Print model info\n",
    "for name, (model, tokenizer) in models.items():\n",
    "    print(f\"\\nModel: {name}\")\n",
    "    print(f\"  - Model type: {type(model).__name__}\")\n",
    "    print(f\"  - Tokenizer type: {type(tokenizer).__name__}\")\n",
    "    print(f\"  - Vocabulary size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcafa3c",
   "metadata": {},
   "source": [
    "## Test Simple Generation\n",
    "\n",
    "Let's test a simple Polish text generation to verify the model is working properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b50f9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the first model\n",
    "model_name = list(models.keys())[0]\n",
    "model, tokenizer = models[model_name]\n",
    "\n",
    "# Define a simple Polish prompt\n",
    "prompt = \"Napisz krótki tekst o języku polskim. Język polski to język, który\"\n",
    "\n",
    "# Get model-specific generation parameters\n",
    "gen_params = configure_generation_parameters(model_name)\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate text\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=150,\n",
    "        **gen_params\n",
    "    )\n",
    "\n",
    "# Decode output\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"Generated text:\\n{generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cbf0e9",
   "metadata": {},
   "source": [
    "## Test Polish GLUE Task Example\n",
    "\n",
    "Let's test a single example from a Polish GLUE task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e60cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a task and sample\n",
    "task_name = next(iter(datasets.keys()))\n",
    "dataset_info = datasets[task_name]\n",
    "sample = dataset_info['dataset'][0]\n",
    "\n",
    "# Create prompt for this task\n",
    "prompt = create_task_specific_prompt(task_name, sample)\n",
    "print(f\"Task: {task_name}\")\n",
    "print(f\"Prompt:\\n{prompt}\")\n",
    "\n",
    "# Generate response\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,\n",
    "        **gen_params\n",
    "    )\n",
    "\n",
    "# Decode output\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"\\nModel response:\\n{generated_text}\")\n",
    "\n",
    "# Extract the actual answer (after the prompt)\n",
    "answer = generated_text.replace(prompt, \"\").strip()\n",
    "print(f\"\\nExtracted answer: {answer}\")\n",
    "\n",
    "# Show the ground truth answer\n",
    "label_key = dataset_info['keys'][-1]  # Usually the last key is the label\n",
    "label_value = sample[label_key]\n",
    "print(f\"Ground truth: {label_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5879d98f",
   "metadata": {},
   "source": [
    "## Run Evaluation\n",
    "\n",
    "Now let's evaluate the model on Polish GLUE tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f3c24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure evaluation parameters\n",
    "metrics = {\n",
    "    \"classification\": [\"accuracy\", \"f1\"],\n",
    "    \"regression\": [\"mse\", \"spearman\"],\n",
    "    \"generation\": [\"bleu\", \"rouge\"]\n",
    "}\n",
    "\n",
    "# Use model-specific generation parameters\n",
    "generation_params = {}\n",
    "for model_name in models.keys():\n",
    "    generation_params[model_name] = configure_generation_parameters(model_name)\n",
    "\n",
    "# Define results directory\n",
    "results_dir = os.path.join(os.path.dirname(os.path.abspath('.')), \"results\")\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "print(f\"Results will be saved to: {results_dir}\")\n",
    "\n",
    "# Run evaluation\n",
    "evaluation_results = evaluate_models(\n",
    "    models=models,\n",
    "    datasets=datasets,\n",
    "    metrics=metrics,\n",
    "    max_samples=None,  # Use all examples in the datasets\n",
    "    generation_params=generation_params,\n",
    "    save_results=True,\n",
    "    results_dir=results_dir\n",
    ")\n",
    "\n",
    "print(\"Evaluation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febe1aeb",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d54c7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary table of results\n",
    "results_summary = []\n",
    "\n",
    "for model_name, model_results in evaluation_results.items():\n",
    "    for task_name, task_results in model_results.items():\n",
    "        metrics = task_results.get('metrics', {})\n",
    "        \n",
    "        for metric_name, score in metrics.items():\n",
    "            results_summary.append({\n",
    "                'Model': model_name,\n",
    "                'Task': task_name,\n",
    "                'Metric': metric_name,\n",
    "                'Score': score\n",
    "            })\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results_summary)\n",
    "\n",
    "# Display the results table\n",
    "display(results_df)\n",
    "\n",
    "# Create visualizations\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Group by task and metric, showing scores for different models\n",
    "for task in results_df['Task'].unique():\n",
    "    task_df = results_df[results_df['Task'] == task]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Metric', y='Score', hue='Model', data=task_df)\n",
    "    plt.title(f'Performance on {task}')\n",
    "    plt.ylim(0, max(1.0, task_df['Score'].max() * 1.1))  # Adjust y-axis limit based on max score\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef3eb89",
   "metadata": {},
   "source": [
    "## Analysis of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f22b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze model performance across all tasks\n",
    "model_summaries = {}\n",
    "\n",
    "for model_name in results_df['Model'].unique():\n",
    "    model_data = results_df[results_df['Model'] == model_name]\n",
    "    \n",
    "    # For metrics like accuracy, f1 where higher is better\n",
    "    higher_better_metrics = ['accuracy', 'f1', 'f1_macro', 'bleu', 'rouge1', 'rouge2', 'rougeL', 'spearman']\n",
    "    higher_scores = model_data[model_data['Metric'].isin(higher_better_metrics)]\n",
    "    \n",
    "    # For metrics like mse, mae where lower is better\n",
    "    lower_better_metrics = ['mse', 'mae']\n",
    "    lower_scores = model_data[model_data['Metric'].isin(lower_better_metrics)]\n",
    "    \n",
    "    model_summaries[model_name] = {\n",
    "        'avg_accuracy': higher_scores[higher_scores['Metric'] == 'accuracy']['Score'].mean(),\n",
    "        'avg_f1': higher_scores[higher_scores['Metric'].str.contains('f1')]['Score'].mean(),\n",
    "        'best_task': higher_scores.loc[higher_scores['Score'].idxmax()]['Task'],\n",
    "        'best_score': higher_scores['Score'].max(),\n",
    "        'worst_task': higher_scores.loc[higher_scores['Score'].idxmin()]['Task'],\n",
    "        'worst_score': higher_scores['Score'].min()\n",
    "    }\n",
    "\n",
    "# Convert to DataFrame for display\n",
    "summary_df = pd.DataFrame.from_dict(model_summaries, orient='index')\n",
    "display(summary_df)\n",
    "\n",
    "# Visualize model comparison across different metrics\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Model', y='Score', hue='Metric', \n",
    "            data=results_df[results_df['Metric'].isin(['accuracy', 'f1_macro'])])\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.ylim(0, 1.0)\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bacd64a",
   "metadata": {},
   "source": [
    "## Save Detailed Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c55d1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive analysis report\n",
    "analysis_report = {\n",
    "    'summary': model_summaries,\n",
    "    'detailed_results': evaluation_results,\n",
    "    'metadata': {\n",
    "        'evaluation_date': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        'models_evaluated': list(models.keys()),\n",
    "        'tasks_evaluated': list(datasets.keys()),\n",
    "        'sample_counts': {task: len(info['dataset']) for task, info in datasets.items()}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "report_path = os.path.join(results_dir, f\"analysis_report_{timestamp}.json\")\n",
    "\n",
    "# Convert any non-serializable objects to strings\n",
    "def convert_for_json(obj):\n",
    "    if isinstance(obj, (np.int64, np.int32, np.float64, np.float32)):\n",
    "        return obj.item()\n",
    "    return str(obj)\n",
    "\n",
    "with open(report_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(analysis_report, f, ensure_ascii=False, indent=2, default=convert_for_json)\n",
    "\n",
    "print(f\"Analysis report saved to: {report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a901ddf4",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Evaluate the other models (Gemma and Bielik)\n",
    "2. Test with more KLEJ benchmark tasks\n",
    "3. Increase the sample size for more robust evaluation\n",
    "4. Perform error analysis on model predictions\n",
    "5. Explore model performance on different categories of examples\n",
    "6. Generate a comprehensive report comparing all three models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
