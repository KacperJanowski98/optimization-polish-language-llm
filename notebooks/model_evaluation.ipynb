{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polish Language Model Evaluation\n",
    "\n",
    "This notebook evaluates three language models on Polish language tasks:\n",
    "\n",
    "1. **Bielik-11B-v2.3-Instruct** - A specialized Polish language model\n",
    "2. **Google Gemma-3-4B-IT** - A multilingual model\n",
    "3. **Microsoft Phi-4-mini-instruct** - A multilingual model\n",
    "\n",
    "We will compare their performance on several Polish language benchmark datasets from the KLEJ benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "First, we'll set up the environment and check that all requirements are met."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path for imports\n",
    "sys.path.append('..')\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger('model_evaluation')\n",
    "\n",
    "# Create results directory if it doesn't exist\n",
    "os.makedirs('../results', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    # Print GPU memory info\n",
    "    gpu_properties = torch.cuda.get_device_properties(0)\n",
    "    print(f\"Total memory: {gpu_properties.total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"CUDA capability: {gpu_properties.major}.{gpu_properties.minor}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU available. Using CPU (this will be very slow for large models).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import project modules\n",
    "from src.utils import check_environment_setup, optimize_memory, check_gpu_compatibility\n",
    "from src.datasets import PolishDatasetLoader, prepare_datasets_for_evaluation\n",
    "from src.init_models import setup_huggingface_auth, initialize_model, MODEL_CONFIGS\n",
    "from src.evaluation import run_evaluation, aggregate_results, visualize_results, generate_summary_report, save_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check environment setup\n",
    "setup_ok = check_environment_setup()\n",
    "if not setup_ok:\n",
    "    print(\"There are issues with your environment setup. See warnings above.\")\n",
    "else:\n",
    "    print(\"Environment setup looks good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Size and Memory Requirements\n",
    "\n",
    "Let's check the memory requirements for each model to determine the best loading strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display model size information and memory requirements\n",
    "model_sizes = {\n",
    "    'bielik': 11,    # 11B parameters\n",
    "    'gemma': 4,      # 4B parameters\n",
    "    'phi': 3.8       # 3.8B parameters (approximate)\n",
    "}\n",
    "\n",
    "for model_key, size in model_sizes.items():\n",
    "    print(f\"\\n{model_key.upper()} - {MODEL_CONFIGS[model_key]['model_id']}\")\n",
    "    print(f\"Size: {size}B parameters\")\n",
    "    print(check_gpu_compatibility(size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets\n",
    "\n",
    "Now, let's load the Polish language datasets for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Hugging Face authentication\n",
    "setup_huggingface_auth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset loader\n",
    "dataset_loader = PolishDatasetLoader(cache_dir='../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of samples per dataset (adjust based on your needs and time constraints)\n",
    "samples_per_dataset = {\n",
    "    'dyk': 50,      # Question-answer correctness\n",
    "    'polemo2': 50,  # Sentiment analysis\n",
    "    'psc': 50,      # Text similarity\n",
    "    'cdsc': 50      # Entailment\n",
    "}\n",
    "\n",
    "# Load and sample datasets\n",
    "evaluation_datasets = dataset_loader.create_evaluation_dataset(\n",
    "    samples_per_dataset=samples_per_dataset,\n",
    "    split='test'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine Dataset Examples\n",
    "\n",
    "Let's look at a few examples from each dataset to understand the tasks better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View examples from DYK dataset\n",
    "dyk_examples = evaluation_datasets['dyk'].select(range(3))\n",
    "for i, example in enumerate(dyk_examples):\n",
    "    print(f\"Example {i+1}\")\n",
    "    print(f\"Question: {example['question']}\")\n",
    "    print(f\"Answer: {example['answer']}\")\n",
    "    print(f\"Target: {example['target']}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View examples from POLEMO dataset\n",
    "polemo_examples = evaluation_datasets['polemo2'].select(range(3))\n",
    "for i, example in enumerate(polemo_examples):\n",
    "    print(f\"Example {i+1}\")\n",
    "    print(f\"Sentence: {example['sentence']}\")\n",
    "    print(f\"Target: {example['target']}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View examples from PSC dataset\n",
    "psc_examples = evaluation_datasets['psc'].select(range(3))\n",
    "for i, example in enumerate(psc_examples):\n",
    "    print(f\"Example {i+1}\")\n",
    "    print(f\"Extract Text: {example['extract_text'][:200]}...\")\n",
    "    print(f\"Summary Text: {example['summary_text']}\")\n",
    "    print(f\"Label: {example['label']}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View examples from CDSC dataset\n",
    "cdsc_examples = evaluation_datasets['cdsc'].select(range(3))\n",
    "for i, example in enumerate(cdsc_examples):\n",
    "    print(f\"Example {i+1}\")\n",
    "    print(f\"Sentence A: {example['sentence_A']}\")\n",
    "    print(f\"Sentence B: {example['sentence_B']}\")\n",
    "    print(f\"Entailment: {example['entailment_judgment']}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Now, let's evaluate each model on the datasets. This will take some time, especially for large models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Bielik-11B-v2.3-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Bielik model (using 8-bit quantization due to its size)\n",
    "print(\"Loading Bielik-11B model...\")\n",
    "bielik_model, bielik_tokenizer = initialize_model(\n",
    "    model_key='bielik',\n",
    "    device=\"cuda\",\n",
    "    load_in_8bit=True,  # Use 8-bit quantization for memory efficiency\n",
    "    cache_dir='../models'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Bielik model\n",
    "bielik_results = run_evaluation(\n",
    "    model_key='bielik',\n",
    "    model=bielik_model,\n",
    "    tokenizer=bielik_tokenizer,\n",
    "    datasets=evaluation_datasets,\n",
    "    device=\"cuda\",\n",
    "    max_samples_per_dataset=None,  # Use all sampled examples\n",
    "    output_dir='../results'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up GPU memory\n",
    "del bielik_model\n",
    "del bielik_tokenizer\n",
    "optimize_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Google Gemma-3-4B-IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Gemma model\n",
    "print(\"Loading Gemma-3-4B model...\")\n",
    "gemma_model, gemma_tokenizer = initialize_model(\n",
    "    model_key='gemma',\n",
    "    device=\"cuda\",\n",
    "    load_in_8bit=False,  # Don't need 8-bit for this smaller model\n",
    "    cache_dir='../models'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Gemma model\n",
    "gemma_results = run_evaluation(\n",
    "    model_key='gemma',\n",
    "    model=gemma_model,\n",
    "    tokenizer=gemma_tokenizer,\n",
    "    datasets=evaluation_datasets,\n",
    "    device=\"cuda\",\n",
    "    max_samples_per_dataset=None,\n",
    "    output_dir='../results'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up GPU memory\n",
    "del gemma_model\n",
    "del gemma_tokenizer\n",
    "optimize_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Microsoft Phi-4-mini-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Phi model\n",
    "print(\"Loading Phi-4-mini model...\")\n",
    "phi_model, phi_tokenizer = initialize_model(\n",
    "    model_key='phi',\n",
    "    device=\"cuda\",\n",
    "    load_in_8bit=False,  # Don't need 8-bit for this smaller model\n",
    "    cache_dir='../models'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Phi model\n",
    "phi_results = run_evaluation(\n",
    "    model_key='phi',\n",
    "    model=phi_model,\n",
    "    tokenizer=phi_tokenizer,\n",
    "    datasets=evaluation_datasets,\n",
    "    device=\"cuda\",\n",
    "    max_samples_per_dataset=None,\n",
    "    output_dir='../results'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up GPU memory\n",
    "del phi_model\n",
    "del phi_tokenizer\n",
    "optimize_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis\n",
    "\n",
    "Now let's analyze the results of all three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all results\n",
    "all_results = {\n",
    "    'bielik': bielik_results,\n",
    "    'gemma': gemma_results,\n",
    "    'phi': phi_results\n",
    "}\n",
    "\n",
    "# Save complete results\n",
    "save_results(\n",
    "    results=all_results,\n",
    "    output_dir='../results',\n",
    "    prefix='full_evaluation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create aggregated results DataFrame\n",
    "results_df = aggregate_results(all_results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "visualize_results(results_df, output_path='../results/performance_comparison.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and display summary report\n",
    "summary_report = generate_summary_report(results_df)\n",
    "print(summary_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary report to file\n",
    "with open(\"../results/summary_report.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(summary_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset-Specific Analysis\n",
    "\n",
    "Let's look at the performance on each dataset separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze performance on DYK dataset\n",
    "dyk_results = results_df[results_df['dataset'] == 'dyk']\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='model', y='f1', data=dyk_results)\n",
    "plt.title('Performance on DYK Dataset (F1 Score)')\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze performance on POLEMO dataset\n",
    "polemo_results = results_df[results_df['dataset'] == 'polemo2']\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='model', y='accuracy', data=polemo_results)\n",
    "plt.title('Performance on POLEMO Dataset (Accuracy)')\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze performance on PSC dataset\n",
    "psc_results = results_df[results_df['dataset'] == 'psc']\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='model', y='f1', data=psc_results)\n",
    "plt.title('Performance on PSC Dataset (F1 Score)')\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze performance on CDSC dataset\n",
    "cdsc_results = results_df[results_df['dataset'] == 'cdsc']\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='model', y='accuracy', data=cdsc_results)\n",
    "plt.title('Performance on CDSC Dataset (Accuracy)')\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison\n",
    "\n",
    "Let's compare the overall performance of the models across all metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average performance per model\n",
    "avg_performance = results_df.groupby('model')[['accuracy', 'f1', 'precision']].mean()\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of overall performance\n",
    "avg_performance_long = avg_performance.reset_index().melt(\n",
    "    id_vars=['model'],\n",
    "    value_vars=['accuracy', 'f1', 'precision'],\n",
    "    var_name='metric',\n",
    "    value_name='score'\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='model', y='score', hue='metric', data=avg_performance_long)\n",
    "plt.title('Overall Model Performance Across Metrics')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.legend(title='Metric')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/overall_performance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Based on the evaluation results, we can draw several conclusions:\n",
    "\n",
    "1. **Overall Performance Comparison**: [To be filled after running the evaluation]\n",
    "2. **Task-Specific Strengths**: [To be filled after running the evaluation]\n",
    "3. **Polish vs. Multilingual Models**: [To be filled after running the evaluation]\n",
    "4. **Size vs. Performance Trade-off**: [To be filled after running the evaluation]\n",
    "\n",
    "The evaluation demonstrates [overall conclusion to be added after running]."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
