{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polish Language Model Evaluation\n",
    "\n",
    "This notebook evaluates four language models on Polish language tasks:\n",
    "\n",
    "1. **Bielik-11B-v2.3-Instruct** - A specialized Polish language model\n",
    "2. **Google Gemma-3-4B-IT** - A multilingual model\n",
    "3. **Microsoft Phi-4-mini-instruct** - A multilingual model\n",
    "4. **CYFRAGOVPL/Llama-PLLuM-8B-instruct** - Polish government-backed Llama-based model\n",
    "\n",
    "We will compare their performance on several Polish language benchmark datasets from the KLEJ benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "First, we'll set up the environment and check that all requirements are met."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path for imports\n",
    "sys.path.append('..')\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger('model_evaluation')\n",
    "\n",
    "# Create results directory if it doesn't exist\n",
    "os.makedirs('../results', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    # Print GPU memory info\n",
    "    gpu_properties = torch.cuda.get_device_properties(0)\n",
    "    print(f\"Total memory: {gpu_properties.total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"CUDA capability: {gpu_properties.major}.{gpu_properties.minor}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU available. Using CPU (this will be very slow for large models).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import project modules\n",
    "from src.utils import check_environment_setup, optimize_memory, check_gpu_compatibility\n",
    "from src.datasets import PolishDatasetLoader, prepare_datasets_for_evaluation\n",
    "from src.init_models import setup_huggingface_auth, initialize_model, MODEL_CONFIGS\n",
    "from src.evaluation import run_evaluation, aggregate_results, visualize_results, generate_summary_report, save_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check environment setup\n",
    "setup_ok = check_environment_setup()\n",
    "if not setup_ok:\n",
    "    print(\"There are issues with your environment setup. See warnings above.\")\n",
    "else:\n",
    "    print(\"Environment setup looks good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Size and Memory Requirements\n",
    "\n",
    "Let's check the memory requirements for each model to determine the best loading strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display model size information and memory requirements\n",
    "model_sizes = {\n",
    "    'bielik': 11,    # 11B parameters\n",
    "    'gemma': 4,      # 4B parameters\n",
    "    'phi': 3.8,      # 3.8B parameters (approximate)\n",
    "    'pllum': 8       # 8B parameters\n",
    "}\n",
    "\n",
    "for model_key, size in model_sizes.items():\n",
    "    print(f\"\\n{model_key.upper()} - {MODEL_CONFIGS[model_key]['model_id']}\")\n",
    "    print(f\"Size: {size}B parameters\")\n",
    "    print(check_gpu_compatibility(size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets\n",
    "\n",
    "Now, let's load the Polish language datasets for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Hugging Face authentication\n",
    "setup_huggingface_auth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset loader\n",
    "dataset_loader = PolishDatasetLoader(cache_dir='../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of samples per dataset (adjust based on your needs and time constraints)\n",
    "samples_per_dataset = {\n",
    "    'dyk': 50,      # Question-answer correctness\n",
    "    'polemo2': 50,  # Sentiment analysis\n",
    "    'psc': 50,      # Text similarity\n",
    "    'cdsc': 50      # Entailment\n",
    "}\n",
    "\n",
    "# Load and sample datasets\n",
    "evaluation_datasets = dataset_loader.create_evaluation_dataset(\n",
    "    samples_per_dataset=samples_per_dataset,\n",
    "    split='test'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine Dataset Examples\n",
    "\n",
    "Let's look at a few examples from each dataset to understand the tasks better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View examples from DYK dataset\n",
    "dyk_examples = evaluation_datasets['dyk'].select(range(3))\n",
    "for i, example in enumerate(dyk_examples):\n",
    "    print(f\"Example {i+1}\")\n",
    "    print(f\"Question: {example['question']}\")\n",
    "    print(f\"Answer: {example['answer']}\")\n",
    "    print(f\"Target: {example['target']}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View examples from POLEMO dataset\n",
    "polemo_examples = evaluation_datasets['polemo2'].select(range(3))\n",
    "for i, example in enumerate(polemo_examples):\n",
    "    print(f\"Example {i+1}\")\n",
    "    print(f\"Sentence: {example['sentence']}\")\n",
    "    print(f\"Target: {example['target']}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View examples from PSC dataset\n",
    "psc_examples = evaluation_datasets['psc'].select(range(3))\n",
    "for i, example in enumerate(psc_examples):\n",
    "    print(f\"Example {i+1}\")\n",
    "    print(f\"Extract Text: {example['extract_text'][:200]}...\")\n",
    "    print(f\"Summary Text: {example['summary_text']}\")\n",
    "    print(f\"Label: {example['label']}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View examples from CDSC dataset\n",
    "cdsc_examples = evaluation_datasets['cdsc'].select(range(3))\n",
    "for i, example in enumerate(cdsc_examples):\n",
    "    print(f\"Example {i+1}\")\n",
    "    print(f\"Sentence A: {example['sentence_A']}\")\n",
    "    print(f\"Sentence B: {example['sentence_B']}\")\n",
    "    print(f\"Entailment: {example['entailment_judgment']}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Now, let's evaluate each model on the datasets. This will take some time, especially for large models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Bielik-11B-v2.3-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Bielik model with CPU offloading\n",
    "print(\"Loading Bielik-11B model with CPU offloading and 4-bit quantization...\")\n",
    "bielik_model, bielik_tokenizer = initialize_model(\n",
    "    model_key='bielik',\n",
    "    device=\"cuda\",\n",
    "    load_in_8bit=False,\n",
    "    load_in_4bit=True,  # 4-bit quantization for memory efficiency\n",
    "    offload_to_cpu=True,  # Enable CPU offloading\n",
    "    cache_dir='../models'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Bielik model\n",
    "bielik_results = run_evaluation(\n",
    "    model_key='bielik',\n",
    "    model=bielik_model,\n",
    "    tokenizer=bielik_tokenizer,\n",
    "    datasets=evaluation_datasets,\n",
    "    device=\"cuda\",\n",
    "    max_samples_per_dataset=None,  # Use all sampled examples\n",
    "    output_dir='../results'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up GPU memory\n",
    "del bielik_model\n",
    "del bielik_tokenizer\n",
    "optimize_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Google Gemma-3-4B-IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Gemma model\n",
    "print(\"Loading Gemma-3-4B model...\")\n",
    "gemma_model, gemma_tokenizer = initialize_model(\n",
    "    model_key='gemma',\n",
    "    device=\"cuda\",\n",
    "    load_in_8bit=False,\n",
    "    load_in_4bit=True,  # 4-bit quantization for memory efficiency\n",
    "    offload_to_cpu=True,  # Enable CPU offloading\n",
    "    cache_dir='../models'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Gemma model\n",
    "gemma_results = run_evaluation(\n",
    "    model_key='gemma',\n",
    "    model=gemma_model,\n",
    "    tokenizer=gemma_tokenizer,\n",
    "    datasets=evaluation_datasets,\n",
    "    device=\"cuda\",\n",
    "    max_samples_per_dataset=None,\n",
    "    output_dir='../results'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up GPU memory\n",
    "del gemma_model\n",
    "del gemma_tokenizer\n",
    "optimize_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Microsoft Phi-4-mini-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Phi model\n",
    "print(\"Loading Phi-4-mini model...\")\n",
    "phi_model, phi_tokenizer = initialize_model(\n",
    "    model_key='phi',\n",
    "    device=\"cuda\",\n",
    "    load_in_8bit=False,\n",
    "    load_in_4bit=True,  # 4-bit quantization for memory efficiency\n",
    "    offload_to_cpu=True,  # Enable CPU offloading\n",
    "    cache_dir='../models'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Phi model\n",
    "phi_results = run_evaluation(\n",
    "    model_key='phi',\n",
    "    model=phi_model,\n",
    "    tokenizer=phi_tokenizer,\n",
    "    datasets=evaluation_datasets,\n",
    "    device=\"cuda\",\n",
    "    max_samples_per_dataset=None,\n",
    "    output_dir='../results'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up GPU memory\n",
    "del phi_model\n",
    "del phi_tokenizer\n",
    "optimize_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: CYFRAGOVPL/Llama-PLLuM-8B-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PLLuM model\n",
    "print(\"Loading PLLuM-8B model...\")\n",
    "pllum_model, pllum_tokenizer = initialize_model(\n",
    "    model_key='pllum',\n",
    "    device=\"cuda\",\n",
    "    load_in_8bit=False,\n",
    "    load_in_4bit=True,  # 4-bit quantization for memory efficiency\n",
    "    offload_to_cpu=True,  # Enable CPU offloading\n",
    "    cache_dir='../models'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate PLLuM model\n",
    "pllum_results = run_evaluation(\n",
    "    model_key='pllum',\n",
    "    model=pllum_model,\n",
    "    tokenizer=pllum_tokenizer,\n",
    "    datasets=evaluation_datasets,\n",
    "    device=\"cuda\",\n",
    "    max_samples_per_dataset=None,\n",
    "    output_dir='../results'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up GPU memory\n",
    "del pllum_model\n",
    "del pllum_tokenizer\n",
    "optimize_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Individual Model Results\n",
    "\n",
    "Let's save the individual model results to JSON files before clearing the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save individual model results\n",
    "def save_model_results(model_key, results, output_dir='../results'):\n",
    "    \"\"\"Save individual model results to a JSON file\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    file_path = os.path.join(output_dir, f\"{model_key}_results.json\")\n",
    "    \n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"Results saved to {file_path}\")\n",
    "\n",
    "# Save results for each model\n",
    "try:\n",
    "    save_model_results('bielik', bielik_results)\n",
    "except NameError:\n",
    "    print(\"Bielik results not available\")\n",
    "    \n",
    "try:\n",
    "    save_model_results('gemma', gemma_results)\n",
    "except NameError:\n",
    "    print(\"Gemma results not available\")\n",
    "    \n",
    "try:\n",
    "    save_model_results('phi', phi_results)\n",
    "except NameError:\n",
    "    print(\"Phi results not available\")\n",
    "    \n",
    "try:\n",
    "    save_model_results('pllum', pllum_results)\n",
    "except NameError:\n",
    "    print(\"PLLuM results not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis\n",
    "\n",
    "Now let's analyze the results from the JSON files rather than in-memory variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load results from JSON files\n",
    "def load_results_from_files(results_dir='../results', pattern='*_results.json'):\n",
    "    \"\"\"Load model evaluation results from JSON files\"\"\"\n",
    "    results = {}\n",
    "    results_files = glob.glob(os.path.join(results_dir, pattern))\n",
    "    \n",
    "    if not results_files:\n",
    "        print(f\"No result files found matching {pattern} in {results_dir}\")\n",
    "        return results\n",
    "    \n",
    "    for file_path in results_files:\n",
    "        try:\n",
    "            model_key = os.path.basename(file_path).split('_')[0]\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                results[model_key] = json.load(f)\n",
    "            print(f\"Loaded results for {model_key} from {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Load all results from JSON files\n",
    "all_results = load_results_from_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which models we have results for\n",
    "print(f\"Results available for models: {list(all_results.keys())}\")\n",
    "\n",
    "# Create aggregated results DataFrame\n",
    "results_df = aggregate_results(all_results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "visualize_results(results_df, output_path='../results/performance_comparison.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and display summary report\n",
    "summary_report = generate_summary_report(results_df)\n",
    "print(summary_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary report to file\n",
    "with open(\"../results/summary_report.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(summary_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset-Specific Analysis\n",
    "\n",
    "Let's look at the performance on each dataset separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze performance on DYK dataset\n",
    "dyk_results = results_df[results_df['dataset'] == 'dyk']\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='model', y='f1', data=dyk_results)\n",
    "plt.title('Performance on DYK Dataset (F1 Score)')\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.savefig('../results/dyk_performance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze performance on POLEMO dataset\n",
    "polemo_results = results_df[results_df['dataset'] == 'polemo2']\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='model', y='accuracy', data=polemo_results)\n",
    "plt.title('Performance on POLEMO Dataset (Accuracy)')\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.savefig('../results/polemo_performance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze performance on PSC dataset\n",
    "psc_results = results_df[results_df['dataset'] == 'psc']\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='model', y='f1', data=psc_results)\n",
    "plt.title('Performance on PSC Dataset (F1 Score)')\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.savefig('../results/psc_performance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze performance on CDSC dataset\n",
    "cdsc_results = results_df[results_df['dataset'] == 'cdsc']\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='model', y='accuracy', data=cdsc_results)\n",
    "plt.title('Performance on CDSC Dataset (Accuracy)')\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.savefig('../results/cdsc_performance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison\n",
    "\n",
    "Let's compare the overall performance of the models across all metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average performance per model\n",
    "avg_performance = results_df.groupby('model')[['accuracy', 'f1', 'precision']].mean()\n",
    "avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of overall performance\n",
    "avg_performance_long = avg_performance.reset_index().melt(\n",
    "    id_vars=['model'],\n",
    "    value_vars=['accuracy', 'f1', 'precision'],\n",
    "    var_name='metric',\n",
    "    value_name='score'\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='model', y='score', hue='metric', data=avg_performance_long)\n",
    "plt.title('Overall Model Performance Across Metrics')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.legend(title='Metric')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/overall_performance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polish-Specific Models Comparison\n",
    "\n",
    "Let's specifically compare the two Polish-dedicated models (Bielik and PLLuM) against the multilingual models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group models by type (Polish-specific vs multilingual)\n",
    "polish_models = ['bielik', 'pllum']\n",
    "multilingual_models = ['gemma', 'phi']\n",
    "\n",
    "# Add model type column\n",
    "results_df['model_type'] = results_df['model'].apply(\n",
    "    lambda x: 'Polish-dedicated' if x in polish_models else 'Multilingual'\n",
    ")\n",
    "\n",
    "# Calculate average performance by model type\n",
    "model_type_performance = results_df.groupby(['model_type', 'dataset'])[\n",
    "    ['accuracy', 'f1', 'precision']\n",
    "].mean().reset_index()\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Plot accuracy by dataset and model type\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.barplot(x='dataset', y='accuracy', hue='model_type', data=model_type_performance)\n",
    "plt.title('Accuracy by Dataset and Model Type')\n",
    "plt.ylim(0, 1)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Plot F1 score by dataset and model type\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.barplot(x='dataset', y='f1', hue='model_type', data=model_type_performance)\n",
    "plt.title('F1 Score by Dataset and Model Type')\n",
    "plt.ylim(0, 1)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/polish_vs_multilingual.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Based on the evaluation results, we can draw several conclusions:\n",
    "\n",
    "1. **Overall Performance Comparison**: [To be filled after running the evaluation]\n",
    "2. **Task-Specific Strengths**: [To be filled after running the evaluation]\n",
    "3. **Polish vs. Multilingual Models**: [To be filled after running the evaluation]\n",
    "4. **Size vs. Performance Trade-off**: [To be filled after running the evaluation]\n",
    "5. **PLLuM vs. Bielik Comparison**: [To be filled after running the evaluation]\n",
    "\n",
    "The evaluation demonstrates [overall conclusion to be added after running]."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}